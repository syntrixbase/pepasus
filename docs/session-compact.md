# Session Compact

Session compact manages Main Agent's conversation history length. When the session grows too long for the LLM's context window, compact archives the current session and replaces it with a summary — preserving continuity without exceeding token limits.

## Why Compact

Main Agent maintains a persistent session (JSONL). Every user message, inner monologue, tool call, and task result appends to this session. Without compaction, the session eventually exceeds the model's context window, causing failures or truncation.

Compact solves this by periodically:
1. Summarizing the conversation so far
2. Archiving the full session to a timestamped file
3. Starting a fresh session with the summary + archive reference

## Trigger Strategy

**When**: checked at the start of every `_think()` call, before the LLM invocation.

**How**: hybrid estimation with zero network overhead.

```
estimatedTokens = lastPromptTokens + estimate(newMessages)
```

- **`lastPromptTokens`** — exact token count from the previous LLM response's `usage.promptTokens` (free, precise)
- **`estimate(newMessages)`** — local estimation of messages added since the last LLM call (tiktoken or `len / 3.5`, no API calls)

**Trigger condition**:

```
if estimatedTokens > contextWindow * compactThreshold → compact
```

**First call**: when no `lastPromptTokens` exists yet (first `_think()` after startup), estimate the entire session locally.

**Why this works**: the exact base (`lastPromptTokens`) carries most of the precision. The delta is typically a few messages, so even rough estimation has negligible error. The threshold buffer (default 20%) absorbs any inaccuracy.

## Token Threshold

### Context Window Sizes

Each model has a known context window. A built-in mapping table provides defaults:

```typescript
// Examples
"gpt-4o":                       128_000
"gpt-4o-mini":                  128_000
"gpt-4.1":                    1_000_000
"gpt-4.1-mini":               1_000_000
"claude-sonnet-4-20250514":    200_000
"claude-opus-4-20250514":      200_000
"claude-haiku-3-20250307":     200_000
```

Unknown models fall back to `128_000`.

### Compact Threshold

Configurable, default **80%** of context window.

```yaml
# config.yml
session:
  compactThreshold: 0.8
```

- Lower value (e.g. 0.6) → compact earlier, more conservative
- Higher value (e.g. 0.9) → compact later, use more context

The 20% buffer accommodates: estimation error, LLM response tokens, and safety margin.

## Summary Generation

### Independent LLM Call

Summary is generated by a **standalone LLM call**, completely separate from Main Agent's `_think()` loop.

- Uses the same model as Main Agent (no separate configuration)
- Has its own system prompt optimized for summarization
- No tools, no inner monologue — pure `messages in → summary out`
- Not injected into Main Agent's cognitive process

**Why independent**: summarization is a system-level operation, not part of Main Agent's reasoning. Mixing it into `_think()` would pollute the inner monologue and complicate the prompt.

### Summary Content

The summary must preserve:

| Keep | Discard |
|------|---------|
| User's current intent & what to do next | Greetings, small talk |
| Key decisions and conclusions reached | Inner monologue reasoning steps |
| Ongoing tasks and their status | Redundant tool call details |
| User preferences and relevant context | Intermediate tool results |

The most critical items are **user's latest intent** and **next steps** — these ensure Main Agent knows what it was doing after compact.

## Compact Flow

```
_think() entry
  │
  ├─ estimate tokens
  │   lastPromptTokens + estimate(new messages)
  │
  ├─ exceeds threshold?
  │   no → proceed to LLM call normally
  │   yes ↓
  │
  ├─ 1. Generate summary (independent LLM call)
  │      system: "Summarize this conversation..."
  │      messages: currentSession
  │      → returns summary text
  │
  ├─ 2. Archive current session
  │      rename current.jsonl → {timestamp}.jsonl
  │
  ├─ 3. Create new session
  │      write summary as first entry with archive reference
  │
  ├─ 4. Reset in-memory state
  │      sessionMessages = [summary message]
  │      lastPromptTokens = 0 (will be set after next LLM call)
  │
  └─ 5. Continue _think() with compacted session
```

## Post-Compact Session Structure

After compact, the new `current.jsonl` starts with a single system message:

```jsonl
{"ts":1740000000,"role":"system","content":"Summary: ...","metadata":{"type":"compact","previousSession":"20260225T143000.jsonl"}}
```

The `previousSession` field references the archived file. Main Agent can read it via the `session_archive_read` tool if the summary lacks sufficient detail.

### File Layout

```
data/main/
├── current.jsonl              ← active session (starts with summary)
├── 20260225T143000.jsonl      ← most recent archive (readable via tool)
├── 20260224T180000.jsonl      ← older archive (kept, not actively referenced)
└── 20260223T120000.jsonl      ← older archive
```

## Archive Retrieval

### `session_archive_read` Tool

A new tool added to Main Agent's curated tool set. Allows Main Agent to read the **previous** archived session when the summary doesn't contain enough detail.

**Scope**: reads only the most recent archive (referenced in the compact metadata). Older archives are not accessible through this tool.

**Why only one level**: the most recent archive contains the conversation that was just compacted — the most likely source of needed detail. Older context should have already been captured by the **memory system** (facts / episodes).

### Relationship to Memory

```
┌─────────────────────────────────────────────┐
│              Time horizon                    │
│                                              │
│  session_archive_read ◄── recent (1 compact) │
│                                              │
│  memory system        ◄── long-term          │
│  (facts / episodes)       (across compacts)  │
└─────────────────────────────────────────────┘
```

- **Session** handles short-term context (current conversation + one archive)
- **Memory** handles long-term knowledge (persists across many compacts)
- Information that matters long-term should be written to memory, not relied upon through archive chains

## Configuration

```yaml
session:
  compactThreshold: 0.8    # fraction of context window (default: 0.8)
```

No separate model configuration for summarization — uses the same model as Main Agent. This can be extended later if cost optimization is needed.
