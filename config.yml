# Pegasus Default Configuration
#
# This is the default configuration file that ships with Pegasus.
# You can edit this file directly, or create config.local.yml for local overrides.
#
# Configuration Loading Strategy:
# 1. This file (config.yml) provides base configuration
# 2. Create config.local.yml for local overrides (gitignored)
# 3. config.local.yml values will deep-merge and override config.yml
# 4. Environment variables will override all file-based configs
#
# Priority (highest to lowest):
#   Environment Variables > config.local.yml > config.yml > Schema Defaults
#
# IMPORTANT: This project uses .yml as the standard extension.
# Do not create config.yaml as it will conflict with this file.
#
# Environment Variable Interpolation:
# Supports bash-style syntax in ${...} placeholders:
#   ${VAR}           - Use environment variable (empty if unset)
#   ${VAR:-default}  - Use default if VAR is unset or empty
#   ${VAR:=default}  - Use and assign default if VAR is unset or empty
#   ${VAR:?error}    - Error if VAR is unset or empty (required)
#   ${VAR:+alternate}- Use alternate if VAR is set

llm:
  # OpenAI Codex integration (Responses API + OAuth)
  # When enabled, Pegasus runs OAuth login at startup to obtain Codex credentials.
  # Use "codex/<model>" in tier mapping, e.g. tiers.fast: codex/gpt-5.3-codex
  codex:
    enabled: ${CODEX_ENABLED:-false}

  # GitHub Copilot integration (OpenAI-compatible API + GitHub OAuth)
  # When enabled, Pegasus runs GitHub device code login at startup to obtain Copilot credentials.
  # Use "copilot/<model>" in tier mapping, e.g. default: copilot/gpt-4o
  copilot:
    enabled: ${COPILOT_ENABLED:-false}

  # Provider configurations — each key becomes a provider name
  # Used in tiers/default as "providerName/modelName"
  providers:
    openai:
      apiKey: ${OPENAI_API_KEY}
      baseURL: ${OPENAI_BASE_URL:-}

    anthropic:
      apiKey: ${ANTHROPIC_API_KEY}
      baseURL: ${ANTHROPIC_BASE_URL:-}

    zai:
      type: ${ZAI_TYPE:-openai}  # treat ZAI as OpenAI-compatible
      apiKey: ${ZAI_API_KEY}
      baseURL: ${ZAI_BASE_URL:-https://api.z.ai/api/coding/paas/v4}

    litellm:
      type: ${LITELLM_TYPE:-openai}
      apiKey: ${LITELLM_API_KEY}
      baseURL: ${LITELLM_BASE_URL:-https://api.litellm.com/v1}

    # For Ollama, LM Studio, or other OpenAI-compatible endpoints
    # Usage: roles.default: "ollama/llama3.2:latest"
    ollama:
      type: openai
      apiKey: dummy
      baseURL: ${OLLAMA_BASE_URL:-http://localhost:11434/v1}

  # Default model — used for MainAgent and as fallback for all tiers.
  # Format: "provider/model" string or object { model, contextWindow?, apiType? }
  default: ${LLM_DEFAULT_MODEL:-openai/gpt-4o}

  # Tier → model mapping (all optional, fall back to default)
  #
  # Tiers define capability levels, not specific use cases.
  # Subagents and skills declare which tier they need (fast/balanced/powerful).
  # Internal tasks (compact, reflection, extract) use the "fast" tier.
  #
  # Each tier can be a shorthand string or an object:
  #   tierName: provider/model
  #   tierName:
  #     model: provider/model
  #     contextWindow: 128000
  #     apiType: openai
  tiers:
    fast: ${LLM_FAST_MODEL:-}                # explore subagent, compact, reflection, extract
    balanced: ${LLM_BALANCED_MODEL:-}        # general subagent
    powerful: ${LLM_POWERFUL_MODEL:-}        # plan subagent, complex reasoning

  # Concurrent request settings
  maxConcurrentCalls: ${LLM_MAX_CONCURRENT_CALLS:-3}
  timeout: ${LLM_TIMEOUT:-120}  # seconds

  # Global context window size (tokens). Auto-detected from model if not set.
  # Acts as fallback when per-role contextWindow is not configured.
  # Override when using providers with different context limits (e.g. OpenRouter).
  contextWindow: ${LLM_CONTEXT_WINDOW:-}

identity:
  personaPath: ${IDENTITY_PERSONA_PATH:-data/personas/default.json}

agent:
  maxActiveTasks: 5
  maxConcurrentTools: 3
  maxCognitiveIterations: ${AGENT_MAX_COGNITIVE_ITERATIONS:-10}
  heartbeatInterval: 60  # seconds
  taskTimeout: 300  # seconds, max wait time for task completion

tools:
  # Tool execution timeout (seconds)
  timeout: ${TOOLS_TIMEOUT:-60}

  # Allowed paths for file operations (empty = no restriction)
  # Supports both absolute paths (/path/to/dir) and relative paths (./data)
  # Relative paths are resolved against dataDir
  allowedPaths: ${TOOLS_ALLOWED_PATHS:-[]}

  # Web search configuration (optional)
  webSearch:
    provider: ${WEB_SEARCH_PROVIDER:-tavily}  # tavily (more providers planned)
    apiKey: ${WEB_SEARCH_API_KEY}
    maxResults: ${WEB_SEARCH_MAX_RESULTS:-5}

  # MCP (Model Context Protocol) server configurations
  # Supports two transports: stdio (local subprocess) and sse (HTTP SSE/StreamableHTTP)
  #
  # stdio example:
  #   mcpServers:
  #     - name: filesystem
  #       transport: stdio
  #       command: npx
  #       args: ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
  #       env: {}
  #       cwd: .
  #       enabled: true
  #
  # sse example:
  #   mcpServers:
  #     - name: remote-tools
  #       transport: sse
  #       url: http://localhost:3000/sse
  #       enabled: true
  mcpServers: ${TOOLS_MCP_SERVERS:-[]}

session:
  # Fraction of context window that triggers compact (0.1-1.0)
  compactThreshold: ${SESSION_COMPACT_THRESHOLD:-0.8}

channels:
  telegram:
    enabled: ${TELEGRAM_ENABLED:-false}
    token: ${TELEGRAM_BOT_TOKEN:-}

system:
  logLevel: ${PEGASUS_LOG_LEVEL:-info}  # debug | info | warn | error
  dataDir: ${PEGASUS_DATA_DIR:-data}
  authDir: ${PEGASUS_AUTH_DIR:-~/.pegasus/auth}

  # Log output format (file only, no console output)
  # json — structured JSON lines, machine-parseable (default)
  # line — human-readable single lines: TIME LEVEL [module] message key=value
  logFormat: ${PEGASUS_LOG_FORMAT:-json}
