# Pegasus Default Configuration
#
# This is the default configuration file that ships with Pegasus.
# You can edit this file directly, or create config.local.yml for local overrides.
#
# Configuration Loading Strategy:
# 1. This file (config.yml) provides base configuration
# 2. Create config.local.yml for local overrides (gitignored)
# 3. config.local.yml values will deep-merge and override config.yml
# 4. Environment variables will override all file-based configs
#
# Priority (highest to lowest):
#   Environment Variables > config.local.yml > config.yml > Schema Defaults
#
# IMPORTANT: This project uses .yml as the standard extension.
# Do not create config.yaml as it will conflict with this file.
#
# Environment Variable Interpolation:
# Supports bash-style syntax in ${...} placeholders:
#   ${VAR}           - Use environment variable (empty if unset)
#   ${VAR:-default}  - Use default if VAR is unset or empty
#   ${VAR:=default}  - Use and assign default if VAR is unset or empty
#   ${VAR:?error}    - Error if VAR is unset or empty (required)
#   ${VAR:+alternate}- Use alternate if VAR is set

llm:
  # Provider configurations — each key becomes a provider name
  # Used in roles as "providerName/modelName"
  providers:
    openai:
      apiKey: ${OPENAI_API_KEY}
      baseURL: ${OPENAI_BASE_URL:-}

    anthropic:
      apiKey: ${ANTHROPIC_API_KEY}
      baseURL: ${ANTHROPIC_BASE_URL:-}

    # For Ollama, LM Studio, or other OpenAI-compatible endpoints
    # Usage: roles.default: "ollama/llama3.2:latest"
    ollama:
      type: openai
      apiKey: dummy
      baseURL: ${OLLAMA_BASE_URL:-http://localhost:11434/v1}

  # Role → model mapping (format: "provider/model")
  # Roles without a value fall back to "default"
  roles:
    default: ${LLM_DEFAULT_MODEL:-openai/gpt-5.2}
    subAgent: ${LLM_SUB_AGENT_MODEL:-}
    compact: ${LLM_COMPACT_MODEL:-}
    reflection: ${LLM_REFLECTION_MODEL:-}

  # Concurrent request settings
  maxConcurrentCalls: ${LLM_MAX_CONCURRENT_CALLS:-3}
  timeout: ${LLM_TIMEOUT:-120}  # seconds

  # Context window size (tokens). Auto-detected from model if not set.
  # Override when using providers with different context limits (e.g. OpenRouter).
  contextWindow: ${LLM_CONTEXT_WINDOW:-}

identity:
  personaPath: ${IDENTITY_PERSONA_PATH:-data/personas/default.json}

agent:
  maxActiveTasks: 5
  maxConcurrentTools: 3
  maxCognitiveIterations: 10
  heartbeatInterval: 60  # seconds
  taskTimeout: 300  # seconds, max wait time for task completion

tools:
  # Tool execution timeout (seconds)
  timeout: ${TOOLS_TIMEOUT:-60}

  # Allowed paths for file operations (empty = no restriction)
  # Supports both absolute paths (/path/to/dir) and relative paths (./data)
  # Relative paths are resolved against dataDir
  allowedPaths: ${TOOLS_ALLOWED_PATHS:-[]}

  # Web search configuration (optional)
  webSearch:
    provider: ${WEB_SEARCH_PROVIDER:-tavily}  # tavily | google | bing | duckduckgo
    apiKey: ${WEB_SEARCH_API_KEY}
    maxResults: 10

  # MCP server configurations (optional, for future use)
  mcpServers: ${TOOLS_MCP_SERVERS:-[]}

session:
  # Fraction of context window that triggers compact (0.1-1.0)
  compactThreshold: ${SESSION_COMPACT_THRESHOLD:-0.8}

system:
  logLevel: ${PEGASUS_LOG_LEVEL:-info}  # debug | info | warn | error
  dataDir: ${PEGASUS_DATA_DIR:-data}

  # Log output format (file only, no console output)
  # json — structured JSON lines, machine-parseable (default)
  # line — human-readable single lines: TIME LEVEL [module] message key=value
  logFormat: ${PEGASUS_LOG_FORMAT:-json}
