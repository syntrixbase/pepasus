# Pegasus Default Configuration
#
# This is the default configuration file that ships with Pegasus.
# You can edit this file directly, or create config.local.yml for local overrides.
#
# Configuration Loading Strategy:
# 1. This file (config.yml) provides base configuration
# 2. Create config.local.yml for local overrides (gitignored)
# 3. config.local.yml values will deep-merge and override config.yml
# 4. Environment variables will override all file-based configs
#
# Priority (highest to lowest):
#   Environment Variables > config.local.yml > config.yml > Schema Defaults
#
# IMPORTANT: This project uses .yml as the standard extension.
# Do not create config.yaml as it will conflict with this file.
#
# Environment Variable Interpolation:
# Supports bash-style syntax in ${...} placeholders:
#   ${VAR}           - Use environment variable (empty if unset)
#   ${VAR:-default}  - Use default if VAR is unset or empty
#   ${VAR:=default}  - Use and assign default if VAR is unset or empty
#   ${VAR:?error}    - Error if VAR is unset or empty (required)
#   ${VAR:+alternate}- Use alternate if VAR is set

llm:
  # OpenAI Codex integration (Responses API + OAuth)
  # When enabled, Pegasus runs OAuth login at startup to obtain Codex credentials.
  # Use "codex/<model>" in role mapping, e.g. roles.subAgent: codex/gpt-5.3-codex
  codex:
    enabled: ${CODEX_ENABLED:-false}

  # GitHub Copilot integration (OpenAI-compatible API + GitHub OAuth)
  # When enabled, Pegasus runs GitHub device code login at startup to obtain Copilot credentials.
  # Use "copilot/<model>" in role mapping, e.g. roles.default: copilot/gpt-4o
  copilot:
    enabled: ${COPILOT_ENABLED:-false}

  # Provider configurations — each key becomes a provider name
  # Used in roles as "providerName/modelName"
  providers:
    openai:
      apiKey: ${OPENAI_API_KEY}
      baseURL: ${OPENAI_BASE_URL:-}

    anthropic:
      apiKey: ${ANTHROPIC_API_KEY}
      baseURL: ${ANTHROPIC_BASE_URL:-}

    zai:
      type: ${ZAI_TYPE:-openai}  # treat ZAI as OpenAI-compatible
      apiKey: ${ZAI_API_KEY}
      baseURL: ${ZAI_BASE_URL:-https://api.z.ai/api/coding/paas/v4}

    litellm:
      type: ${LITELLM_TYPE:-openai}
      apiKey: ${LITELLM_API_KEY}
      baseURL: ${LITELLM_BASE_URL:-https://api.litellm.com/v1}

    # For Ollama, LM Studio, or other OpenAI-compatible endpoints
    # Usage: roles.default: "ollama/llama3.2:latest"
    ollama:
      type: openai
      apiKey: dummy
      baseURL: ${OLLAMA_BASE_URL:-http://localhost:11434/v1}

  # Role → model mapping
  # Each role can be either a shorthand string or an object with extra options.
  #
  # Shorthand (string):
  #   roleName: provider/model
  #
  # Extended (object):
  #   roleName:
  #     model: provider/model            # required
  #     contextWindow: 128000            # optional — override auto-detected context window size
  #     apiType: openai                  # optional — override provider's SDK type (openai | anthropic)
  #
  # Roles without a value fall back to "default".
  #
  # Context window resolution priority:
  #   Per-role contextWindow > Global llm.contextWindow > Auto-detect from model > 128k default
  #
  # Examples:
  #   roles:
  #     default: openai/gpt-4o                              # shorthand
  #     subAgent:
  #       model: myhost/claude-sonnet-4                     # custom provider
  #       contextWindow: 200000                             # explicit context window
  #       apiType: anthropic                                # override SDK type
  #     compact: openai/gpt-4o-mini                         # shorthand
  #     reflection:
  #       model: openai/gpt-4o-mini
  #       contextWindow: 16000                              # smaller window for cost savings
  roles:
    default: ${LLM_DEFAULT_MODEL:-openai/gpt-4o}
    subAgent: ${LLM_SUB_AGENT_MODEL:-}
    compact: ${LLM_COMPACT_MODEL:-}
    reflection: ${LLM_REFLECTION_MODEL:-}

  # Concurrent request settings
  maxConcurrentCalls: ${LLM_MAX_CONCURRENT_CALLS:-3}
  timeout: ${LLM_TIMEOUT:-120}  # seconds

  # Global context window size (tokens). Auto-detected from model if not set.
  # Acts as fallback when per-role contextWindow is not configured.
  # Override when using providers with different context limits (e.g. OpenRouter).
  contextWindow: ${LLM_CONTEXT_WINDOW:-}

identity:
  personaPath: ${IDENTITY_PERSONA_PATH:-data/personas/default.json}

agent:
  maxActiveTasks: 5
  maxConcurrentTools: 3
  maxCognitiveIterations: ${AGENT_MAX_COGNITIVE_ITERATIONS:-10}
  heartbeatInterval: 60  # seconds
  taskTimeout: 300  # seconds, max wait time for task completion

tools:
  # Tool execution timeout (seconds)
  timeout: ${TOOLS_TIMEOUT:-60}

  # Allowed paths for file operations (empty = no restriction)
  # Supports both absolute paths (/path/to/dir) and relative paths (./data)
  # Relative paths are resolved against dataDir
  allowedPaths: ${TOOLS_ALLOWED_PATHS:-[]}

  # Web search configuration (optional)
  webSearch:
    provider: ${WEB_SEARCH_PROVIDER:-tavily}  # tavily | google | bing | duckduckgo
    apiKey: ${WEB_SEARCH_API_KEY}
    maxResults: 10

  # MCP (Model Context Protocol) server configurations
  # Supports two transports: stdio (local subprocess) and sse (HTTP SSE/StreamableHTTP)
  #
  # stdio example:
  #   mcpServers:
  #     - name: filesystem
  #       transport: stdio
  #       command: npx
  #       args: ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
  #       env: {}
  #       cwd: .
  #       enabled: true
  #
  # sse example:
  #   mcpServers:
  #     - name: remote-tools
  #       transport: sse
  #       url: http://localhost:3000/sse
  #       enabled: true
  mcpServers: ${TOOLS_MCP_SERVERS:-[]}

session:
  # Fraction of context window that triggers compact (0.1-1.0)
  compactThreshold: ${SESSION_COMPACT_THRESHOLD:-0.8}

channels:
  telegram:
    enabled: ${TELEGRAM_ENABLED:-false}
    token: ${TELEGRAM_BOT_TOKEN:-}

system:
  logLevel: ${PEGASUS_LOG_LEVEL:-info}  # debug | info | warn | error
  dataDir: ${PEGASUS_DATA_DIR:-data}
  authDir: ${PEGASUS_AUTH_DIR:-~/.pegasus/auth}

  # Log output format (file only, no console output)
  # json — structured JSON lines, machine-parseable (default)
  # line — human-readable single lines: TIME LEVEL [module] message key=value
  logFormat: ${PEGASUS_LOG_FORMAT:-json}
