# ============================================
# Pegasus Configuration
# ============================================
#
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ============================================
# LLM Configuration
# ============================================

# Active LLM Provider (REQUIRED)
# Options: openai, anthropic, openai-compatible
# Default: openai
LLM_PROVIDER=openai

# Default model name (used if provider-specific model not set)
# Default: gpt-4o-mini
LLM_MODEL=gpt-4o-mini

# ============================================
# OpenAI Provider Configuration
# ============================================

# OpenAI API Key (REQUIRED when LLM_PROVIDER=openai)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-key-here

# OpenAI Model (OPTIONAL - defaults to LLM_MODEL)
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# OPENAI_MODEL=gpt-4o-mini

# OpenAI Base URL (OPTIONAL - defaults to official API)
# Use for OpenAI-compatible proxies or custom endpoints
# OPENAI_BASE_URL=https://api.openai.com/v1

# ============================================
# Anthropic Provider Configuration
# ============================================

# Anthropic API Key (REQUIRED when LLM_PROVIDER=anthropic)
# Get from: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=your-anthropic-key-here

# Anthropic Model (OPTIONAL - defaults to LLM_MODEL)
# Options: claude-sonnet-4-20250514, claude-opus-4-20250514, claude-3-5-sonnet-20241022
# ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Anthropic Base URL (OPTIONAL - defaults to official API)
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# ============================================
# OpenAI-Compatible Provider (Ollama, LM Studio, etc.)
# ============================================

# Base URL (REQUIRED when LLM_PROVIDER=openai-compatible)
# Examples:
#   Ollama:     http://localhost:11434/v1
#   LM Studio:  http://localhost:1234/v1
#   Together:   https://api.together.xyz/v1
#   vLLM:       http://your-server:8000/v1
# LLM_BASE_URL=http://localhost:11434/v1

# Model name for compatible provider (use OPENAI_MODEL or LLM_MODEL)
# For Ollama: llama3.2:latest, qwen2.5:latest, etc.
# OPENAI_MODEL=llama3.2:latest

# API Key (OPTIONAL - many local models don't need it)
# OPENAI_API_KEY=dummy

# ============================================
# System-wide LLM Settings
# ============================================

# Max concurrent LLM calls (default: 3)
# LLM_MAX_CONCURRENT_CALLS=3

# LLM call timeout in seconds (default: 120)
# LLM_TIMEOUT=120

# ============================================
# Example Configurations
# ============================================

# ─────────────────────────────────────────
# Example 1: OpenAI (Default - Cost-effective)
# ─────────────────────────────────────────
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-...
# OPENAI_MODEL=gpt-4o-mini

# ─────────────────────────────────────────
# Example 2: OpenAI with GPT-4o (More capable)
# ─────────────────────────────────────────
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-...
# OPENAI_MODEL=gpt-4o

# ─────────────────────────────────────────
# Example 3: Anthropic Claude (Best reasoning)
# ─────────────────────────────────────────
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-api03-...
# ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ─────────────────────────────────────────
# Example 4: Ollama (Free, Local, No API key)
# ─────────────────────────────────────────
# LLM_PROVIDER=openai-compatible
# LLM_BASE_URL=http://localhost:11434/v1
# OPENAI_MODEL=llama3.2:latest
# # No API key needed for Ollama

# ─────────────────────────────────────────
# Example 5: LM Studio (Free, Local, GUI)
# ─────────────────────────────────────────
# LLM_PROVIDER=openai-compatible
# LLM_BASE_URL=http://localhost:1234/v1
# OPENAI_MODEL=your-loaded-model-name

# ─────────────────────────────────────────
# Example 6: OpenAI via Custom Proxy
# ─────────────────────────────────────────
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your-key
# OPENAI_BASE_URL=https://your-proxy.com/v1
# OPENAI_MODEL=gpt-4o-mini

# ─────────────────────────────────────────
# Example 7: Multiple Providers (Switch by changing LLM_PROVIDER)
# ─────────────────────────────────────────
# LLM_PROVIDER=openai  # Change this to switch
# OPENAI_API_KEY=sk-proj-...
# OPENAI_MODEL=gpt-4o-mini
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ============================================
# Identity Configuration
# ============================================

# Path to persona configuration file (default: data/personas/default.json)
# IDENTITY_PERSONA_PATH=data/personas/default.json

# ============================================
# Agent Configuration
# ============================================

# Max active tasks (default: 5)
# AGENT_MAX_ACTIVE_TASKS=5

# Max concurrent tool calls (default: 3)
# AGENT_MAX_CONCURRENT_TOOLS=3

# Max cognitive loop iterations (default: 10)
# AGENT_MAX_COGNITIVE_ITERATIONS=10

# Heartbeat interval in seconds (default: 60)
# AGENT_HEARTBEAT_INTERVAL=60

# ============================================
# Memory Configuration (for M2)
# ============================================

# SQLite database path (default: data/memory.db)
# MEMORY_DB_PATH=data/memory.db

# Vector database path (default: data/vectors)
# MEMORY_VECTOR_DB_PATH=data/vectors

# ============================================
# System Configuration
# ============================================

# Log level: debug, info, warn, error, silent (default: info)
# PEGASUS_LOG_LEVEL=info

# Data directory (default: data)
# PEGASUS_DATA_DIR=data
