# ============================================
# Pegasus Configuration
# ============================================
#
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# These environment variables are interpolated into config.yml via ${VAR:-default} syntax.
# See config.yml for the full configuration structure.
# See docs/configuration.md for detailed documentation.

# ============================================
# LLM Provider API Keys
# ============================================

# OpenAI API Key
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-key-here

# Anthropic API Key (optional — only if using Anthropic models)
# Get from: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=your-anthropic-key-here

# Custom OpenAI-compatible provider (optional)
# ZAI_API_KEY=your-zai-key-here

# ============================================
# Model Selection
# ============================================

# Default model in "provider/model" format
# This is the model used for all roles unless overridden below.
# Examples:
#   openai/gpt-4o          — OpenAI GPT-4o
#   openai/gpt-4o-mini     — OpenAI GPT-4o Mini (cost-effective)
#   anthropic/claude-sonnet-4-20250514  — Anthropic Claude Sonnet
#   ollama/llama3.2:latest  — Local Ollama model
LLM_DEFAULT_MODEL=openai/gpt-4o

# Per-role model overrides (optional — defaults to LLM_DEFAULT_MODEL)
# LLM_SUB_AGENT_MODEL=openai/gpt-4o-mini
# LLM_COMPACT_MODEL=openai/gpt-4o-mini
# LLM_REFLECTION_MODEL=openai/gpt-4o-mini

# ============================================
# Provider Base URLs (optional overrides)
# ============================================

# Override default API endpoints (e.g., for proxies or regional endpoints)
# OPENAI_BASE_URL=https://api.openai.com/v1
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# Ollama base URL (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1

# ============================================
# LLM Settings
# ============================================

# Max concurrent LLM calls (default: 3)
# LLM_MAX_CONCURRENT_CALLS=3

# LLM call timeout in seconds (default: 120)
# LLM_TIMEOUT=120

# Context window size override in tokens (auto-detected from model if not set)
# LLM_CONTEXT_WINDOW=128000

# ============================================
# System Configuration
# ============================================

# Log level: debug, info, warn, error (default: info)
# PEGASUS_LOG_LEVEL=info

# Data directory (default: data)
# PEGASUS_DATA_DIR=data

# Log format: json (structured) or line (human-readable) (default: json)
# PEGASUS_LOG_FORMAT=json

# Session compact threshold — fraction of context window (default: 0.8)
# SESSION_COMPACT_THRESHOLD=0.8

# ============================================
# Identity
# ============================================

# Path to persona configuration file (default: data/personas/default.json)
# IDENTITY_PERSONA_PATH=data/personas/default.json

# ============================================
# Tools
# ============================================

# Tool execution timeout in seconds (default: 60)
# TOOLS_TIMEOUT=60

# Web search provider and API key (optional)
# WEB_SEARCH_PROVIDER=tavily
# WEB_SEARCH_API_KEY=your-search-key

# ============================================
# Example Configurations
# ============================================

# ── Example 1: OpenAI (recommended) ──
# OPENAI_API_KEY=sk-proj-...
# LLM_DEFAULT_MODEL=openai/gpt-4o

# ── Example 2: Anthropic Claude ──
# ANTHROPIC_API_KEY=sk-ant-api03-...
# LLM_DEFAULT_MODEL=anthropic/claude-sonnet-4-20250514

# ── Example 3: Ollama (free, local, no API key) ──
# LLM_DEFAULT_MODEL=ollama/llama3.2:latest
# No API key needed — Ollama provider uses dummy key

# ── Example 4: Mixed models (different models per role) ──
# OPENAI_API_KEY=sk-proj-...
# LLM_DEFAULT_MODEL=openai/gpt-4o
# LLM_SUB_AGENT_MODEL=openai/gpt-4o-mini
# LLM_COMPACT_MODEL=openai/gpt-4o-mini
# LLM_REFLECTION_MODEL=openai/gpt-4o-mini
