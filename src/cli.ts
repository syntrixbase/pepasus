/**
 * CLI â€” Interactive REPL for conversing with the Pegasus agent.
 *
 * Uses Bun's native readline for zero-dependency terminal interaction.
 */
import { createInterface } from "readline";
import type { LanguageModel } from "./infra/llm-types.ts";
import { Agent } from "./agent.ts";
import { loadPersona } from "./identity/persona.ts";
import { getSettings, getActiveProviderConfig } from "./infra/config.ts";
import { getLogger } from "./infra/logger.ts";
import { createOpenAICompatibleModel } from "./infra/openai-client.ts";
import { createAnthropicCompatibleModel } from "./infra/anthropic-client.ts";
import { TaskState } from "./task/states.ts";

const logger = getLogger("cli");

/** Create a language model from settings. */
function createModel(settings: ReturnType<typeof getSettings>): LanguageModel {
  const { provider } = settings.llm;
  const config = getActiveProviderConfig(settings);

  switch (provider) {
    case "anthropic": {
      if (!config.apiKey) {
        throw new Error(
          "ANTHROPIC_API_KEY is required. Set it in .env:\n" +
            "  ANTHROPIC_API_KEY=sk-ant-api03-your-key-here",
        );
      }

      const model = createAnthropicCompatibleModel({
        apiKey: config.apiKey,
        baseURL: config.baseURL,
        model: config.model,
        headers: {
          "Copilot-Integration-Id": "vscode-chat",
        },
      });

      logger.info({ provider, model: config.model, baseURL: config.baseURL }, "llm_initialized");
      return model;
    }

    case "openai": {
      if (!config.apiKey) {
        throw new Error(
          "OPENAI_API_KEY is required. Set it in .env:\n" +
            "  OPENAI_API_KEY=sk-proj-your-key-here",
        );
      }

      const model = createOpenAICompatibleModel({
        apiKey: config.apiKey,
        baseURL: config.baseURL,
        model: config.model,
        headers: {
          "Copilot-Integration-Id": "vscode-chat",
        },
      });

      logger.info({ provider, model: config.model, baseURL: config.baseURL }, "llm_initialized");
      return model;
    }

    case "openai-compatible": {
      if (!config.baseURL) {
        throw new Error(
          "LLM_BASE_URL is required for openai-compatible provider. Set it in .env:\n" +
            "  LLM_BASE_URL=http://localhost:11434/v1  # For Ollama\n" +
            "  LLM_BASE_URL=http://localhost:1234/v1   # For LM Studio",
        );
      }

      const model = createOpenAICompatibleModel({
        apiKey: config.apiKey || "dummy", // Many local models don't need real key
        baseURL: config.baseURL,
        model: config.model,
      });

      logger.info(
        { provider, model: config.model, baseURL: config.baseURL },
        "llm_initialized",
      );
      return model;
    }

    default:
      throw new Error(`Unsupported LLM provider: ${provider}`);
  }
}

/** Print a styled banner. */
function printBanner(personaName: string, personaRole: string) {
  console.log("");
  console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
  console.log("â•‘          ğŸš€ Pegasus CLI              â•‘");
  console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log(`  Persona: ${personaName} (${personaRole})`);
  console.log("  Type /help for commands, /exit to quit");
  console.log("");
}

/** Handle slash commands. Returns true if command was handled. */
function handleCommand(input: string): boolean | "exit" {
  const cmd = input.trim().toLowerCase();

  if (cmd === "/exit" || cmd === "/quit") {
    console.log("\nğŸ‘‹ Goodbye!\n");
    return "exit";
  }

  if (cmd === "/help") {
    console.log("");
    console.log("  Commands:");
    console.log("    /help   â€” Show this help message");
    console.log("    /exit   â€” Exit the REPL");
    console.log("");
    return true;
  }

  return false;
}

/** Main CLI REPL loop. */
export async function startCLI(): Promise<void> {
  const settings = getSettings();
  const persona = loadPersona(settings.identity.personaPath);
  const model = createModel(settings);

  const agent = new Agent({ model, persona, settings });
  await agent.start();

  printBanner(persona.name, persona.role);

  const rl = createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  rl.setPrompt("> ");

  rl.on("line", async (input) => {
    const trimmed = input.trim();

    // Skip empty input
    if (!trimmed) {
      rl.prompt();
      return;
    }

    // Handle slash commands
    if (trimmed.startsWith("/")) {
      const result = handleCommand(trimmed);
      if (result === "exit") {
        rl.close();
        await agent.stop();
        return;
      }
      if (result === true) {
        rl.prompt();
        return;
      }
      // Not a recognized command â€” treat as regular input
    }

    try {
      const taskId = await agent.submit(trimmed);
      if (!taskId) {
        console.log("  [Error] Failed to create task\n");
        rl.prompt();
        return;
      }

      console.log(`  [Task ${taskId.slice(0, 8)}] Processing...\n`);

      agent.onTaskComplete(taskId, (task) => {
        const result = task.context.finalResult as Record<string, unknown> | null;
        const response = result?.["response"] as string | undefined;

        if (task.state === TaskState.FAILED) {
          console.log(`\n  [Error] Task failed: ${task.context.error}\n`);
        } else if (response) {
          console.log(`\n  ${persona.name}: ${response}\n`);
        } else {
          console.log(`\n  ${persona.name}: [No response generated]\n`);
        }
        rl.prompt(); // Re-show prompt after async output
      });
    } catch (err) {
      logger.error({ error: err }, "cli_error");
      console.log(`  [Error] ${(err as Error).message}\n`);
    }

    rl.prompt(); // Immediately accept next input
  });

  rl.prompt(); // Initial prompt
}

// Entry point: run CLI when this file is executed directly
if (import.meta.main) {
  startCLI().catch((err) => {
    console.error("Fatal error:", err);
    process.exit(1);
  });
}
