# Pegasus Configuration Example
# Copy to config.yaml or config.local.yaml and customize

llm:
  # Active provider: openai | anthropic | openai-compatible
  provider: openai

  # Provider-specific configurations
  providers:
    openai:
      apiKey: ${OPENAI_API_KEY}
      model: gpt-4o-mini
      baseURL: null  # Optional: override API endpoint

    anthropic:
      apiKey: ${ANTHROPIC_API_KEY}
      model: claude-sonnet-4-20250514
      baseURL: null  # Optional: override API endpoint

    # For Ollama, LM Studio, or other OpenAI-compatible endpoints
    ollama:
      apiKey: dummy  # Most local models don't need a real key
      model: llama3.2:latest
      baseURL: http://localhost:11434/v1

  # Concurrent request settings
  maxConcurrentCalls: 3
  timeout: 120  # seconds

identity:
  personaPath: data/personas/default.json

agent:
  maxActiveTasks: 5
  maxConcurrentTools: 3
  maxCognitiveIterations: 10
  heartbeatInterval: 60  # seconds

memory:
  dbPath: data/memory.db
  vectorDbPath: data/vectors

system:
  logLevel: info  # debug | info | warn | error
  dataDir: data
